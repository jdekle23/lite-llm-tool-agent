{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwfW6mW6HGaajZ5CduYl/Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdekle23/lite-llm-tool-agent/blob/main/ai_agent_with_comments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fX4ATdGaE2pX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ai_agent.py\n",
        "\n",
        "A minimal example of a tool‑using AI agent that can:\n",
        "1. List files in the current working directory\n",
        "2. Read the contents of a file\n",
        "3. Terminate its own execution gracefully\n",
        "\n",
        "The agent leverages LiteLLM's `completion` endpoint and demonstrates how to pass\n",
        "structured tool definitions to an LLM, interpret the returned `tool_calls`, and\n",
        "feed the tool responses back into the conversation loop.\n",
        "\n",
        "Usage:\n",
        "$ python ai_agent.py\n",
        "> What would you like me to do? read the README\n",
        "\"\"\"\n",
        "!!pip install litellm\n",
        "\n",
        "import json\n",
        "import os\n",
        "from typing import List, Dict, Callable\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                  OPTIONAL: Pull API key from Google Colab userdata          #\n",
        "# --------------------------------------------------------------------------- #\n",
        "# These four lines let the script run in a Colab notebook where your OpenAI\n",
        "# key is stored in `userdata`. If the import fails (e.g. you are *not* running\n",
        "# inside Colab) the exception is silently ignored and the script falls back to\n",
        "# whatever environment variable or auth method you normally use.\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "\n",
        "    api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "    if api_key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "except Exception:\n",
        "    # ImportError (not Colab) or no key found — do nothing\n",
        "    pass\n",
        "\n",
        "from litellm import completion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------- #\n",
        "#                             Tool Implementations                            #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "def list_files() -> List[str]:\n",
        "    \"\"\"\n",
        "    Return a list of file and directory names located in the current working\n",
        "    directory.\n",
        "\n",
        "    The function is intentionally simple because the agent—not a human—will\n",
        "    parse the returned list and decide which file to read next.\n",
        "    \"\"\"\n",
        "    return os.listdir(\".\")\n",
        "\n",
        "\n",
        "def read_file(file_name: str) -> str:\n",
        "    \"\"\"\n",
        "    Read and return the UTF‑8 text of *file_name*.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_name : str\n",
        "        The relative or absolute path to the file you want to read.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The file contents if successful, otherwise a human‑readable error\n",
        "        message that the agent can pass back to the user.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
        "            return file.read()\n",
        "    except FileNotFoundError:\n",
        "        return f\"Error: {file_name} not found.\"\n",
        "    except Exception as exc:  # Catch‑all so the agent never crashes\n",
        "        return f\"Error: {str(exc)}\"\n",
        "\n",
        "\n",
        "def terminate(message: str) -> None:\n",
        "    \"\"\"\n",
        "    Gracefully end the agent loop, printing *message* as a human‑friendly\n",
        "    summary. In production you might return the message instead of printing.\n",
        "    \"\"\"\n",
        "    print(f\"Termination message: {message}\")\n",
        "\n",
        "    # Map each tool name to the underlying Python callable ---------------------- #\n",
        "tool_functions: Dict[str, Callable] = {\n",
        "    \"list_files\": list_files,\n",
        "    \"read_file\": read_file,\n",
        "    \"terminate\": terminate,\n",
        "}\n"
      ],
      "metadata": {
        "id": "12G2kyUwFDLx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------- #\n",
        "#                       OpenAI / LiteLLM Tool Descriptors                     #\n",
        "# --------------------------------------------------------------------------- #\n",
        "# These descriptors are sent as part of every LLM request so that the model\n",
        "# understands the signature and purpose of each available function.\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"list_files\",\n",
        "            \"description\": \"Returns a list of files in the current directory.\",\n",
        "            # No parameters required\n",
        "            \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []},\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"read_file\",\n",
        "            \"description\": \"Reads the content of a specified file.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\"file_name\": {\"type\": \"string\"}},\n",
        "                \"required\": [\"file_name\"],\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"terminate\",\n",
        "            \"description\": (\n",
        "                \"Stops the conversation loop and returns a final message to \"\n",
        "                \"the user.\"\n",
        "            ),\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\"message\": {\"type\": \"string\"}},\n",
        "                \"required\": [\"message\"],\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "MYFjMG7cFMQu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------- #\n",
        "#                               System Prompt                                 #\n",
        "# --------------------------------------------------------------------------- #\n",
        "agent_rules = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"You are an AI agent that can perform tasks by invoking the tools \"\n",
        "            \"I have provided.\\n\\n\"\n",
        "            \"Workflow guidelines:\\n\"\n",
        "            \"1️⃣ If the user asks about files, *first* call list_files to show \"\n",
        "            \"them what is available.\\n\"\n",
        "            \"2️⃣ Only after listing may you call read_file.\\n\"\n",
        "            \"3️⃣ When the task is complete, finish by calling terminate with a \"\n",
        "            \"helpful summary.\"\n",
        "        ),\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "a_FZrJIJFXyi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------- #\n",
        "#                            Agent Loop Parameters                            #\n",
        "# --------------------------------------------------------------------------- #\n",
        "iterations = 0               # Number of completed LLM calls\n",
        "max_iterations = 10          # Safety valve to prevent runaway loops\n",
        "\n",
        "user_task = input(\"What would you like me to do? \")\n",
        "memory = [{\"role\": \"user\", \"content\": user_task}]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94T0PBPrFcg0",
        "outputId": "ed0a8611-318b-43b9-c384-c8327667b18a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What would you like me to do? Which files do I have?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------- #\n",
        "#                                   Loop                                      #\n",
        "# --------------------------------------------------------------------------- #\n",
        "while iterations < max_iterations:\n",
        "    iterations += 1\n",
        "    messages = agent_rules + memory\n",
        "\n",
        "    # Ask the LLM what to do next ------------------------------------------- #\n",
        "    response = completion(\n",
        "        model=\"openai/gpt-4o\",\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        max_tokens=1024,\n",
        "    )\n",
        "\n",
        "    # The model either calls a tool or replies directly --------------------- #\n",
        "    tool_calls = response.choices[0].message.tool_calls\n",
        "    if tool_calls:\n",
        "        # For simplicity, we only execute the first tool call in this example\n",
        "        tool = tool_calls[0]\n",
        "        tool_name = tool.function.name\n",
        "        tool_args = json.loads(tool.function.arguments)\n",
        "\n",
        "        print(f\"\\n→ Executing: {tool_name}({tool_args})\")\n",
        "\n",
        "        if tool_name == \"terminate\":\n",
        "            # terminate() handles its own printing\n",
        "            terminate(**tool_args)\n",
        "            break\n",
        "\n",
        "        # Execute the Python function and capture its result ---------------- #\n",
        "        try:\n",
        "            result_payload = tool_functions[tool_name](**tool_args)\n",
        "            result = {\"result\": result_payload}\n",
        "        except Exception as exc:\n",
        "            result = {\"error\": f\"Error executing {tool_name}: {str(exc)}\"}\n",
        "\n",
        "        print(f\"← Result: {result}\")\n",
        "\n",
        "        # Feed the tool invocation and the result back into the conversation #\n",
        "        memory.extend(\n",
        "            [\n",
        "                {\"role\": \"assistant\", \"content\": json.dumps({\"tool_name\": tool_name, \"args\": tool_args})},\n",
        "                {\"role\": \"user\", \"content\": json.dumps(result)},\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        # No tool call: the model sent a final natural‑language answer ------ #\n",
        "        print(f\"\\nAssistant response: {response.choices[0].message.content}\")\n",
        "        break\n",
        "else:\n",
        "    # Loop exited because we hit max_iterations ----------------------------- #\n",
        "    print(\"Max iterations reached without termination.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIFKWPBPFiTM",
        "outputId": "d5f3fcfa-335e-41b3-d4c0-2e4b929c0fba"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Executing: list_files({})\n",
            "← Result: {'result': ['.config', 'sample_data']}\n",
            "\n",
            "Assistant response: It looks like you have a directory named `.config` and another named `sample_data`. If you need information about the contents of these directories or specific files, please let me know!\n"
          ]
        }
      ]
    }
  ]
}